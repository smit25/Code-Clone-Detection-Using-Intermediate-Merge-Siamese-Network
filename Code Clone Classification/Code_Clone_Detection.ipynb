{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Code Clone Detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1xIyy_DVK6U"
      },
      "source": [
        "Mount Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3M0o1roUkhH",
        "outputId": "9751b631-ed62-4b20-a92b-8afcab57de1f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4trtLG7WXbrt"
      },
      "source": [
        "Get Data From Csv File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvbddHeSVQ4E"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Dataset Length: 300201 \n",
        "def get_csv_data(csv_path, feature_len = 32):\n",
        "  csv_data = pd.read_csv(csv_path, sep=',', header=None)\n",
        "  data = csv_data.values.astype(np.float)[:, 0:2*feature_len]\n",
        "  labels = csv_data.values.astype(np.float)[:,2*feature_len:]\n",
        "\n",
        "  total_data = np.hstack((data, labels))\n",
        "\n",
        "  np.random.shuffle(total_data)\n",
        "\n",
        "  shuffled_data = total_data[:, :-1]\n",
        "  shuffled_labels = total_data[:, -1]\n",
        "\n",
        "  left_data = shuffled_data[:, 0:feature_len]\n",
        "  right_data = shuffled_data[:, feature_len: 2*feature_len]\n",
        "  \n",
        "  return left_data, right_data, shuffled_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GbaazqvdjBV"
      },
      "source": [
        "Dataloader for the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ljgjflIdiZx"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.utils.data as Data\n",
        "\n",
        "class Dataloader(Data.Dataset):\n",
        "  def __init__(self, left_arr, right_arr, labels):\n",
        "    super(Dataloader).__init__()\n",
        "\n",
        "    self.left_tensor = torch.from_numpy(left_arr).float()\n",
        "    self.right_tensor = torch.from_numpy(right_arr).float()\n",
        "    self.label = torch.from_numpy(labels).long()\n",
        "    self.len = len(labels)\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.len\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    return (self.left_tensor[idx], self.right_tensor[idx], self.label[idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0iYjvj2Kh_x"
      },
      "source": [
        "Contrastive Loss Function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXRerzv4Kg17"
      },
      "source": [
        "import torch\n",
        "import torch.nn\n",
        "import torch.nn.functional as F\n",
        "class ContrastiveLoss(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, margin=2.0):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, output1, output2, label):\n",
        "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
        "        pos = (1-label) * torch.pow(euclidean_distance, 2)\n",
        "        neg = (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n",
        "        loss_contrastive = torch.mean( pos + neg )\n",
        "        return loss_contrastive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opyw9dYQ--Hb"
      },
      "source": [
        "Fully Connected Siamese NN Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30iu2w5I-hg9"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torch import optim\n",
        "import numpy as np\n",
        "\n",
        "# input: [batch_size, in_channels, features]\n",
        "class Late_Merge_Siamese(nn.Module):\n",
        "  def __init__(self, hyperparam, activ_bool = True):\n",
        "    super(Late_Merge_Siamese, self).__init__()\n",
        "\n",
        "    self.activ_func = nn.ReLU(inplace = activ_bool)\n",
        "\n",
        "    self.cnn_1 = nn.Sequential(\n",
        "      nn.Conv1d(1, hyperparam['cnn_1'], kernel_size = 3),\n",
        "      self.activ_func,\n",
        "      #nn.MaxPool1d(hyperparam['max_pool_1'])\n",
        "    )\n",
        "\n",
        "    self.cnn_2 = nn.Sequential(\n",
        "      nn.Conv1d(hyperparam['cnn_1'], hyperparam['cnn_2'], kernel_size = 3),\n",
        "      self.activ_func,\n",
        "      nn.BatchNorm1d(num_features = 64),\n",
        "      nn.Dropout(0.2),\n",
        "      nn.MaxPool1d(hyperparam['max_pool_2'])\n",
        "    )\n",
        "\n",
        "    # Add avg adaptive pooling and see\n",
        "    self.linear = nn.Sequential(\n",
        "      nn.Linear(14*hyperparam['cnn_2'], hyperparam['lin_1']),\n",
        "      self.activ_func,\n",
        "      nn.Linear(hyperparam['lin_1'], hyperparam['lin_2']),\n",
        "      self.activ_func,\n",
        "      nn.Dropout(0.2),\n",
        "      nn.Linear(hyperparam['lin_2'], hyperparam['lin_3']),\n",
        "    )\n",
        "\n",
        "  def forward_once(self, x):\n",
        "    x1 = self.cnn_1(x)\n",
        "    x2 = self.cnn_2(x1)\n",
        "    #print(x2.shape)\n",
        "\n",
        "    x2_out = x2.view(x2.size()[0], -1)\n",
        "    out = self.linear(x2_out)\n",
        "\n",
        "    return out\n",
        "\n",
        "  def forward(self, left, right):\n",
        "    out2 = self.forward_once(right)\n",
        "    out1 = self.forward_once(left)\n",
        "\n",
        "    return out1, out2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zig2cnkDU-6L"
      },
      "source": [
        "Second Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_IJXg3LljEc"
      },
      "source": [
        "class Inter_Merge_Siamese(nn.Module):\n",
        "  def __init__(self, hyperparam, activ_bool = False):\n",
        "    super(Inter_Merge_Siamese, self).__init__()\n",
        "\n",
        "    self.activ_func = nn.LeakyReLU(inplace=activ_bool)\n",
        "\n",
        "    self.cnn_1 = nn.Sequential(\n",
        "      nn.Conv1d(1, hyperparam['cnn_1'], kernel_size = 3),\n",
        "      self.activ_func,\n",
        "      #nn.MaxPool2d(hyperparam['max_pool_1'])\n",
        "    )\n",
        "\n",
        "    self.cnn_2 = nn.Sequential(\n",
        "      nn.Conv1d(hyperparam['cnn_1'], hyperparam['cnn_2'], kernel_size = 3),\n",
        "      self.activ_func,\n",
        "      nn.BatchNorm1d(num_features = 64),\n",
        "      nn.Dropout(0.2),\n",
        "      nn.MaxPool1d(hyperparam['max_pool_2'])\n",
        "    )\n",
        "\n",
        "    self.linear = nn.Sequential(\n",
        "      nn.Linear(14*hyperparam['cnn_2'], hyperparam['lin_1']),\n",
        "      self.activ_func,\n",
        "      nn.Linear(hyperparam['lin_1'], hyperparam['lin_2']),\n",
        "      self.activ_func,\n",
        "      nn.Dropout(0.2),\n",
        "      nn.Linear(hyperparam['lin_2'], hyperparam['lin_3']),\n",
        "      self.activ_func,\n",
        "    )\n",
        "    self.fc = nn.Linear(hyperparam['lin_3'], 2)\n",
        "\n",
        "  def forward_once(self, x):\n",
        "    x1 = self.cnn_1(x)\n",
        "    x2 = self.cnn_2(x1)\n",
        "    #print(x2.shape)\n",
        "\n",
        "    x2_out = x2.view(x2.size()[0], -1)\n",
        "    lin_out = self.linear(x2_out)\n",
        "\n",
        "    return lin_out\n",
        "\n",
        "  def forward(self, left, right):\n",
        "    out1 = self.forward_once(left)\n",
        "    out2 = self.forward_once(right)\n",
        "\n",
        "    out_diff = torch.abs(out1 - out2)\n",
        "    out = self.fc(out_diff)\n",
        "\n",
        "    return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qNk0ZvBXcbn"
      },
      "source": [
        "class NoSiamese(nn.Module):\n",
        "  def __init__(self, hyperparam, activ_bool = False):\n",
        "    super(NoSiamese, self).__init__()\n",
        "\n",
        "    self.activ_func = nn.ReLU(inplace = activ_bool)\n",
        "\n",
        "    self.cnn_1 = nn.Sequential(\n",
        "      nn.Conv1d(1, hyperparam['cnn_1'], kernel_size = 3),\n",
        "      self.activ_func,\n",
        "      #nn.MaxPool2d(hyperparam['max_pool_1'])\n",
        "    )\n",
        "\n",
        "    self.cnn_2 = nn.Sequential(\n",
        "      nn.Conv1d(hyperparam['cnn_1'], hyperparam['cnn_2'], kernel_size = 3),\n",
        "      self.activ_func,\n",
        "      nn.BatchNorm1d(num_features = 64),\n",
        "      nn.Dropout(0.2),\n",
        "      nn.MaxPool1d(hyperparam['max_pool_2'])\n",
        "    )\n",
        "\n",
        "    self.linear = nn.Sequential(\n",
        "      nn.Linear(30*hyperparam['cnn_2'], hyperparam['lin_1']),\n",
        "      self.activ_func,\n",
        "      nn.Linear(hyperparam['lin_1'], hyperparam['lin_2']),\n",
        "      self.activ_func,\n",
        "      nn.Dropout(0.2),\n",
        "      nn.Linear(hyperparam['lin_2'], hyperparam['lin_3']),\n",
        "      self.activ_func,\n",
        "    )\n",
        "    self.fc = nn.Linear(hyperparam['lin_3'], 2)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x1 = self.cnn_1(x)\n",
        "    x2 = self.cnn_2(x1)\n",
        "    x2_out = x2.view(x2.size()[0], -1)\n",
        "    out_lin = self.linear(x2_out)\n",
        "    out = self.fc(out_lin)\n",
        "\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMhS8gq9Lbpk"
      },
      "source": [
        "Install TensorboardX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSX7-m83LbTf",
        "outputId": "2f557777-1778-4153-b395-df9569e46f48"
      },
      "source": [
        "!pip install TensorboardX"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting TensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/84/46421bd3e0e89a92682b1a38b40efc22dafb6d8e3d947e4ceefd4a5fabc7/tensorboardX-2.2-py2.py3-none-any.whl (120kB)\n",
            "\r\u001b[K     |██▊                             | 10kB 21.0MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 20kB 28.6MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 30kB 22.8MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 40kB 18.2MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 51kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 61kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 71kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 81kB 10.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 92kB 11.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 102kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 112kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 122kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from TensorboardX) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from TensorboardX) (3.12.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->TensorboardX) (56.1.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->TensorboardX) (1.15.0)\n",
            "Installing collected packages: TensorboardX\n",
            "Successfully installed TensorboardX-2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9veoqPdRLj_f"
      },
      "source": [
        "Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_Ws_GFRsnzy"
      },
      "source": [
        "def threshold_sigmoid(t):\n",
        "  \"\"\"prob > 0.5 --> 1 else 0\"\"\"\n",
        "  threashold = t.clone()\n",
        "  threashold.data.fill_(0.5)\n",
        "  return (t > threashold).float()\n",
        "\n",
        "def threshold_contrastive(input1, input2, margin = 2.0):\n",
        "    \"\"\"dist < m --> 1 else 0\"\"\"\n",
        "    diff = input1 - input2\n",
        "    dist_sq = torch.sum(torch.pow(diff, 2), 1)\n",
        "    dist = torch.sqrt(dist_sq)\n",
        "    threshold = dist.clone()\n",
        "    threshold.data.fill_(margin)\n",
        "    return (dist < threshold).float().view(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ce138SweWSRg"
      },
      "source": [
        "def count(T):\n",
        "  return torch.count_nonzero(T).item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HruVtN85LmM6"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import os\n",
        "import torch.utils.data as Data\n",
        "from tensorboardX import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, roc_auc_score\n",
        "\n",
        "\"\"\"\n",
        "To view the data written by tensorboardX\n",
        "tensorboard --logdir <path of logs directory>\n",
        "In my case, pathdir = 'logs/'\n",
        "\"\"\"\n",
        "\n",
        "#os.makedirs('/drive/MyDrive', SAVE_DIR, exist_ok=True)\n",
        "\n",
        "hyperparam = {\n",
        "    'cnn_1': 32,\n",
        "    'cnn_2': 64,\n",
        "    'max_pool_1': 2,\n",
        "    'max_pool_2': 2,\n",
        "    'lin_1': 512,\n",
        "    'lin_2': 256,\n",
        "    'lin_3': 128,      \n",
        "  }\n",
        "\n",
        "\n",
        "def init_weights(model):\n",
        "  for name, param in model.named_parameters():\n",
        "    nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "\n",
        "def train(left, right, labels, arch, contra_loss = True):\n",
        "  data = Dataloader(left, right, labels)\n",
        "  logger = SummaryWriter(os.path.join(HOME, LOG_DIR, TIME + ': Code Clone'))\n",
        "  dataset_len = len(labels)\n",
        "\n",
        "  opt = {\n",
        "      'batch_sz': 100,\n",
        "      'lr': 0.0001,\n",
        "      'epochs': 30,\n",
        "      'momentum': 0.09,\n",
        "      'train_len': int(0.70*dataset_len),\n",
        "      'val_len': int(0.85*dataset_len),\n",
        "      'test_len': int(dataset_len),\n",
        "  }\n",
        "\n",
        "  architecture_dict = {\n",
        "    1: Late_Merge_Siamese(hyperparam),\n",
        "    2: NoSiamese(hyperparam),\n",
        "    3: Inter_Merge_Siamese(hyperparam)\n",
        "}\n",
        "\n",
        "  train_loss_arr = []\n",
        "  val_loss_arr = []\n",
        "\n",
        "  feature_len = 32\n",
        "  model = architecture_dict[arch]\n",
        "  #model = Inter_Merge_Siamese(hyperparam)\n",
        "  #model.apply(init_weights)\n",
        "\n",
        "  train_loader = Data.DataLoader(Data.Subset(data, range(opt['train_len'])), batch_size = opt['batch_sz'], shuffle = True)\n",
        "  val_loader = Data.DataLoader(Data.Subset(data, range(opt['train_len'], opt['val_len'])), batch_size = opt['batch_sz'] ,shuffle = True)\n",
        "  test_loader = Data.DataLoader(Data.Subset(data, range(opt['val_len'], opt['test_len'])), batch_size = opt['batch_sz'],shuffle = True)\n",
        "\n",
        "  optimizer = optim.Adam(model.parameters(), lr = opt['lr'])\n",
        "  #scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "  \n",
        "  if arch == 1:\n",
        "    loss_fn = ContrastiveLoss()\n",
        "  else:\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "  \n",
        "  #loss_fn = nn.CrossEntropyLoss()\n",
        "  model.cuda()\n",
        "\n",
        "  print('-----------------BEGIN TRAINING-------------------')\n",
        "\n",
        "  for epoch in range(opt['epochs']):\n",
        "    train_loss = 0.0\n",
        "    model.train()\n",
        "    a = list(model.parameters())[0].clone()\n",
        "    train_len = 0.0\n",
        "    val_len = 0.0\n",
        "\n",
        "    for left_vec, right_vec, label in train_loader:\n",
        "      \n",
        "      left_vec, right_vec= torch.unsqueeze(left_vec, 1), torch.unsqueeze(right_vec, 1)\n",
        "      left_vec = left_vec.to(device)\n",
        "      right_vec = right_vec.to(device)\n",
        "      label = label.to(device)\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      if arch == 1: # LATE_MERGE_SIAMESE\n",
        "        out1, out2 = model(left_vec, right_vec)\n",
        "        loss = loss_fn(out1, out2, label)\n",
        "      elif arch == 2: # NO_SIAMESE\n",
        "        cat_vec = torch.cat((left_vec, right_vec),2)\n",
        "        out = model(cat_vec)\n",
        "        loss = loss_fn(out, label)\n",
        "      else: # INTERMEDIATE_MERGE_SIAMESE\n",
        "        out = model(left_vec, right_vec)\n",
        "        loss = loss_fn(out, label)\n",
        "      \n",
        "      #out = model(left_vec, right_vec)\n",
        "      #loss = loss_fn(out,label)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      train_loss += loss.item()\n",
        "      train_len += label.shape[0]\n",
        "\n",
        "    b = list(model.parameters())[0].clone()\n",
        "    compare = torch.equal(a.data,b.data)\n",
        "    print('BOOL: ', compare)\n",
        "    logger.add_scalar('Training_loss', train_loss/train_len, epoch+1)\n",
        "    print()\n",
        "    print('EPOCH: ', epoch, '---', train_loss/train_len)\n",
        "    train_loss_arr.append(train_loss/train_len)\n",
        "\n",
        "    print('---------------------------BEGIN VALIDATION---------------------------')\n",
        "    \n",
        "    val_loss = 0.0\n",
        "    val_acc = 0.0\n",
        "    \n",
        "    temp = True\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for left_vec, right_vec, label in val_loader:\n",
        "        left_vec, right_vec = torch.unsqueeze(left_vec, 1), torch.unsqueeze(right_vec, 1)\n",
        "        left_vec = left_vec.to(device)\n",
        "        right_vec = right_vec.to(device)\n",
        "        label = label.to(device)\n",
        "        \n",
        "        if arch == 1: # LATE_MERGE_LOSS\n",
        "          out1, out2 = model(left_vec, right_vec)\n",
        "          loss = loss_fn(out1, out2, label)\n",
        "\n",
        "          if contra_loss:\n",
        "            output_labels = threshold_contrastive(out1, out2)\n",
        "          else:\n",
        "            eucledian_distance = F.pairwise_distance(out1, out2)\n",
        "            output_labels = torch.sigmoid(eucledian_distance)\n",
        "\n",
        "        elif arch == 2: # NO_SIAMESE\n",
        "          cat_vec = torch.cat((left_vec, right_vec),2)\n",
        "          out = model(cat_vec)\n",
        "          output_labels = torch.max(out, 1)[1]\n",
        "\n",
        "        else: # INTERMEDIATE_MERGE_SIAMESE\n",
        "          out = model(left_vec, right_vec)\n",
        "          loss = loss_fn(out, label)\n",
        "          output_labels = torch.max(out, 1)[1]\n",
        "        \n",
        "        #out = model(left_vec, right_vec)\n",
        "        #loss = loss_fn(out, label)\n",
        "        #output_labels = torch.max(out, 1)[1]\n",
        "        label = torch.squeeze(label)\n",
        "        output_labels = torch.squeeze(output_labels)\n",
        "        pred = output_labels.data.cpu().numpy()\n",
        "        target = label.data.cpu().numpy()\n",
        "\n",
        "        if temp:\n",
        "          #print('OUT: ', output_labels)\n",
        "          print('OUT2: ', output_labels.shape)\n",
        "          print('OUT3: ', label.shape)\n",
        "          #print('OUTPUT: ', out.shape)\n",
        "          print('OUT_ONES: ', count(output_labels))\n",
        "          print('OUT_LABELS: ', count(label))\n",
        "          print('TORCH: ', float((pred == target).sum()))\n",
        "          temp = False\n",
        "  \n",
        "        old_val_acc = val_acc\n",
        "        val_len += label.shape[0]\n",
        "        val_acc += float((pred == target).sum())\n",
        "        \n",
        "        val_loss += loss.item()\n",
        "\n",
        "    #print('VAL:', val_loss)\n",
        "    print(f'Epoch {epoch+0:03}: | Train Loss: {train_loss/train_len:.5f} | Val Loss: {val_loss/val_len:.5f} | Val Acc: {val_acc/val_len:.3f}')\n",
        "    torch.cuda.empty_cache()\n",
        "    val_loss_arr.append(val_loss/val_len)\n",
        "\n",
        "  plt.figure(figsize=(10,5))\n",
        "  plt.title(\"Training and Validation Loss\")\n",
        "  plt.plot(val_loss_arr,label=\"val\")\n",
        "  plt.plot(train_loss_arr,label=\"train\")\n",
        "  plt.xlabel(\"iterations\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  logger.close()\n",
        "  torch.save({'state_dict': model.state_dict()}, os.path.join(HOME, 'fusional_snn.pt'))\n",
        "  print('TRAINING DONE')\n",
        "  test(model, device, test_loader, arch, contra_loss)\n",
        "  \n",
        "      \n",
        "if __name__ == '__main__':\n",
        "   LOG_DIR = 'logs'\n",
        "   HOME = '/drive/Mydrive'\n",
        "   SAVE_DIR = 'save'\n",
        "   TIME = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "   device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhiHWwJ80Yhw"
      },
      "source": [
        "def test(model, device, test_loader, arch, contra_loss):\n",
        "  model.eval().to(device)\n",
        "  y = {'Actual': [], 'Predicted': []}\n",
        "  with torch.no_grad():\n",
        "    for left_vec, right_vec, label in test_loader:\n",
        "      left_vec, right_vec = torch.unsqueeze(left_vec, 1), torch.unsqueeze(right_vec, 1)\n",
        "      left_vec = left_vec.to(device)\n",
        "      right_vec = right_vec.to(device)\n",
        "      label = label.to(device)\n",
        "      \n",
        "      if arch == 1:\n",
        "        out1, out2 = model(left_vec, right_vec)\n",
        "        if contra_loss:\n",
        "          output_labels = threshold_contrastive(out1, out2)\n",
        "        else:\n",
        "          eucledian_distance = F.pairwise_distance(out1, out2)\n",
        "          output_labels = torch.sigmoid(eucledian_distance)\n",
        "      elif arch == 2:\n",
        "        cat_vec = torch.cat((left_vec, right_vec), 2)\n",
        "        out = model(cat_vec)\n",
        "        output_labels = torch.max(out, 1)[1]\n",
        "      else:\n",
        "        out = model(left_vec, right_vec)\n",
        "        output_labels = torch.max(out, 1)[1]\n",
        "      \n",
        "      #out = model(left_vec, right_vec)\n",
        "      #output_labels = torch.max(out, 1)[1]\n",
        "      label = torch.squeeze(label)\n",
        "      output_labels = torch.squeeze(output_labels)\n",
        "      pred = output_labels.data.cpu().numpy()\n",
        "      target = label.data.cpu().numpy()\n",
        "\n",
        "      y['Actual'].extend(target.tolist())\n",
        "      y['Predicted'].extend(pred.tolist())\n",
        "\n",
        "  print('\\n f1 Score= %.4f' % f1_score(y['Actual'], y['Predicted']))\n",
        "  print('Precision= %.4f' % precision_score(y['Actual'], y['Predicted'], zero_division=0))\n",
        "  print(' Recall= %.4f' % recall_score(y['Actual'], y['Predicted'])) \n",
        " \n",
        "  print('\\nAccuracy: %.4f' % accuracy_score(y['Actual'], y['Predicted'])) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTdANHcGMB9c"
      },
      "source": [
        "Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXTNZiG_ZxFa"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "  csv_path = '/content/drive/MyDrive/Code Clone Detection/syntax_semantic.csv'\n",
        "  left_numpy, right_numpy, labels = get_csv_data(csv_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1g-5Nxd3ALvb",
        "outputId": "042dbffd-4faa-406b-de65-ff208fac9741"
      },
      "source": [
        "CUDA_LAUNCH_BLOCKING=1\n",
        "\n",
        "\"\"\"\n",
        "Late Merge Siamese : 1\n",
        "No Siamese : 2\n",
        "Intermediate Merge Siamese: 3\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "train(left_numpy, right_numpy, labels, 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------BEGIN TRAINING-------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXXTSJ00FQf5"
      },
      "source": [
        "EXTRA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkLrqSeZPr9Q",
        "outputId": "88319f7e-576f-4632-c301-f819fe8b6340"
      },
      "source": [
        "def extra(labels):\n",
        "  ones, zeros = 0,0\n",
        "  for i in labels:\n",
        "    if int(i[0]) == 1:\n",
        "      ones+=1\n",
        "    else:\n",
        "      zeros+=1\n",
        "  return ones, zeros\n",
        "one, zero = extra(labels)\n",
        "print('ones: ', one)\n",
        "print('zeros: ', zero)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ones:  150100\n",
            "zeros:  150101\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}